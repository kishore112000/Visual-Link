{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ci3iac66uoRj"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r yolov5/requirements.txt"
      ],
      "metadata": {
        "id": "HBZPCXXXuyD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from IPython.display import Image  # for displaying images\n",
        "import os \n",
        "import random\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xml.etree.ElementTree as ET\n",
        "from xml.dom import minidom\n",
        "from tqdm import tqdm\n",
        "from PIL import Image, ImageDraw\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(108)"
      ],
      "metadata": {
        "id": "scfqiF8mvE24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ProductDataset.zip"
      ],
      "metadata": {
        "id": "nz3hGRpRvpHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the data from XML Annotation\n",
        "def extract_info_from_xml(xml_file):\n",
        "    root = ET.parse(xml_file).getroot()\n",
        "    \n",
        "    # Initialise the info dict \n",
        "    info_dict = {}\n",
        "    info_dict['bboxes'] = []\n",
        "\n",
        "    # Parse the XML Tree\n",
        "    for elem in root:\n",
        "        # Get the file name \n",
        "        if elem.tag == \"filename\":\n",
        "            info_dict['filename'] = elem.text\n",
        "            \n",
        "        # Get the image size\n",
        "        elif elem.tag == \"size\":\n",
        "            image_size = []\n",
        "            for subelem in elem:\n",
        "                image_size.append(int(subelem.text))\n",
        "            \n",
        "            info_dict['image_size'] = tuple(image_size)\n",
        "        \n",
        "        # Get details of the bounding box \n",
        "        elif elem.tag == \"object\":\n",
        "            bbox = {}\n",
        "            for subelem in elem:\n",
        "                if subelem.tag == \"name\":\n",
        "                    bbox[\"class\"] = subelem.text\n",
        "                    \n",
        "                elif subelem.tag == \"bndbox\":\n",
        "                    for subsubelem in subelem:\n",
        "                        bbox[subsubelem.tag] = int(subsubelem.text)            \n",
        "            info_dict['bboxes'].append(bbox)\n",
        "    \n",
        "    return info_dict"
      ],
      "metadata": {
        "id": "526KB01Y3FeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(extract_info_from_xml('annotations/handbag4.xml'))"
      ],
      "metadata": {
        "id": "zuP9iLGT7i82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the info dict to the required yolo format and write it to disk\n",
        "def convert_to_yolov5(info_dict):\n",
        "    print_buffer = []\n",
        "    \n",
        "    # For each bounding box\n",
        "    for b in info_dict[\"bboxes\"]:\n",
        "        try:\n",
        "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
        "        except KeyError:\n",
        "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
        "        \n",
        "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
        "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2 \n",
        "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
        "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
        "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
        "        \n",
        "        # Normalise the co-ordinates by the dimensions of the image\n",
        "        image_w, image_h, image_c = info_dict[\"image_size\"]  \n",
        "        b_center_x /= image_w \n",
        "        b_center_y /= image_h \n",
        "        b_width    /= image_w \n",
        "        b_height   /= image_h \n",
        "        \n",
        "        #Write the bbox details to the file \n",
        "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
        "        \n",
        "    # Name of the file which we have to save \n",
        "    save_file_name = os.path.join(\"annotations\", info_dict[\"filename\"].replace(\"png\", \"txt\"))\n",
        "    \n",
        "    # Save the annotation to disk\n",
        "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))\n"
      ],
      "metadata": {
        "id": "dzYb9Apy8HBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the annotations\n",
        "annotations = [os.path.join('annotations', x) for x in os.listdir('annotations') if x[-3:] == \"xml\"]\n",
        "annotations.sort()\n",
        "\n",
        "# Convert and save the annotations\n",
        "for ann in tqdm(annotations):\n",
        "    info_dict = extract_info_from_xml(ann)\n",
        "    convert_to_yolov5(info_dict)\n",
        "annotations = [os.path.join('annotations', x) for x in os.listdir('annotations') if x[-3:] == \"txt\"]"
      ],
      "metadata": {
        "id": "TZDUgmOf8J01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(0)\n",
        "\n",
        "class_id_to_name_mapping = dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys()))\n",
        "\n",
        "def plot_bounding_box(image, annotation_list):\n",
        "    annotations = np.array(annotation_list)\n",
        "    w, h = image.size\n",
        "    \n",
        "    plotted_image = ImageDraw.Draw(image)\n",
        "\n",
        "    transformed_annotations = np.copy(annotations)\n",
        "    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w\n",
        "    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h \n",
        "    \n",
        "    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)\n",
        "    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)\n",
        "    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]\n",
        "    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]\n",
        "    \n",
        "    for ann in transformed_annotations:\n",
        "        obj_cls, x0, y0, x1, y1 = ann\n",
        "        plotted_image.rectangle(((x0,y0), (x1,y1)))\n",
        "        \n",
        "        plotted_image.text((x0, y0 - 10), class_id_to_name_mapping[(int(obj_cls))])\n",
        "    \n",
        "    plt.imshow(np.array(image))\n",
        "    plt.show()\n",
        "\n",
        "# Get any random annotation file \n",
        "annotation_file = random.choice(annotations)\n",
        "with open(annotation_file, \"r\") as file:\n",
        "    annotation_list = file.read().split(\"\\n\")[:-1]\n",
        "    annotation_list = [x.split(\" \") for x in annotation_list]\n",
        "    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
        "\n",
        "#Get the corresponding image file\n",
        "image_file = annotation_file.replace(\"annotations\", \"images\").replace(\"txt\", \"png\")\n",
        "assert os.path.exists(image_file)\n",
        "\n",
        "#Load the image\n",
        "image = Image.open(image_file)\n",
        "\n",
        "#Plot the Bounding Box\n",
        "plot_bounding_box(image, annotation_list)"
      ],
      "metadata": {
        "id": "iPE1gTc-8O6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read images and annotations\n",
        "images = [os.path.join('images', x) for x in os.listdir('images')]\n",
        "annotations = [os.path.join('annotations', x) for x in os.listdir('annotations') if x[-3:] == \"txt\"]\n",
        "\n",
        "images.sort()\n",
        "annotations.sort()\n",
        "\n",
        "# Split the dataset into train-valid-test splits \n",
        "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
        "val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)"
      ],
      "metadata": {
        "id": "g-SCSpqK8R37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir images/train images/val images/test annotations/train annotations/val annotations/test"
      ],
      "metadata": {
        "id": "I-4kRvv-8Trz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Utility function to move images \n",
        "def move_files_to_folder(list_of_files, destination_folder):\n",
        "    for f in list_of_files:\n",
        "        try:\n",
        "            shutil.move(f, destination_folder)\n",
        "        except:\n",
        "            print(f)\n",
        "            assert False\n",
        "\n",
        "# Move the splits into their folders\n",
        "move_files_to_folder(train_images, 'images/train')\n",
        "move_files_to_folder(val_images, 'images/val/')\n",
        "move_files_to_folder(test_images, 'images/test/')\n",
        "move_files_to_folder(train_annotations, 'annotations/train/')\n",
        "move_files_to_folder(val_annotations, 'annotations/val/')\n",
        "move_files_to_folder(test_annotations, 'annotations/test/')"
      ],
      "metadata": {
        "id": "iPsyyQRn8VpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv annotations labels\n",
        "!cd ../yolov5 \n"
      ],
      "metadata": {
        "id": "rCPB_mGl8iHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters for COCO training from scratch\n",
        "# python train.py --batch 40 --cfg yolov5m.yaml --weights '' --data coco.yaml --img 640 --epochs 300\n",
        "# See tutorials for hyperparameter evolution https://github.com/ultralytics/yolov5#tutorials\n",
        "\n",
        "\n",
        "lr0: 0.01  # initial learning rate (SGD=1E-2, Adam=1E-3)\n",
        "lrf: 0.2  # final OneCycleLR learning rate (lr0 * lrf)\n",
        "momentum: 0.937  # SGD momentum/Adam beta1\n",
        "weight_decay: 0.0005  # optimizer weight decay 5e-4\n",
        "warmup_epochs: 3.0  # warmup epochs (fractions ok)\n",
        "warmup_momentum: 0.8  # warmup initial momentum\n",
        "warmup_bias_lr: 0.1  # warmup initial bias lr\n",
        "box: 0.05  # box loss gain\n",
        "cls: 0.5  # cls loss gain\n",
        "cls_pw: 1.0  # cls BCELoss positive_weight\n",
        "obj: 1.0  # obj loss gain (scale with pixels)\n",
        "obj_pw: 1.0  # obj BCELoss positive_weight\n",
        "iou_t: 0.20  # IoU training threshold\n",
        "anchor_t: 4.0  # anchor-multiple threshold\n",
        "# anchors: 3  # anchors per output layer (0 to ignore)\n",
        "fl_gamma: 0.0  # focal loss gamma (efficientDet default gamma=1.5)\n",
        "hsv_h: 0.015  # image HSV-Hue augmentation (fraction)\n",
        "hsv_s: 0.7  # image HSV-Saturation augmentation (fraction)\n",
        "hsv_v: 0.4  # image HSV-Value augmentation (fraction)\n",
        "degrees: 0.0  # image rotation (+/- deg)\n",
        "translate: 0.1  # image translation (+/- fraction)\n",
        "scale: 0.5  # image scale (+/- gain)\n",
        "shear: 0.0  # image shear (+/- deg)\n",
        "perspective: 0.0  # image perspective (+/- fraction), range 0-0.001\n",
        "flipud: 0.0  # image flip up-down (probability)\n",
        "fliplr: 0.5  # image flip left-right (probability)\n",
        "mosaic: 1.0  # image mosaic (probability)\n",
        "mixup: 0.0  # image mixup (probability)"
      ],
      "metadata": {
        "id": "22PZNRWl8nQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --img 640 --cfg yolov5s.yaml --hyp hyp.scratch.yaml --batch 32 --epochs 100 --data product_data.yaml --weights yolov5m.pt --workers 24 --name yolo_product_det"
      ],
      "metadata": {
        "id": "K74au0qt81-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}